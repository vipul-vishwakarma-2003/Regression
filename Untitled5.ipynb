{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Regression\n",
        "\n",
        "---\n",
        "\n",
        "1. What is Simple Linear Regression?\n",
        "  - simple linear regression shows the relationship between two variables X and Y, Y is a dependent variable which depends on X which is an independent variable.\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "  - Linearity: The relationship between and must be linear. ...\n",
        "  - Independence of errors: There is not a relationship between the residuals and the variable; in other words, is independent of errors. ...\n",
        "  - Normality of errors: The residuals must be approximately normally distributed.\n",
        "\n",
        "3. What does the coefficient m represent in the equation Y=mX+c ?\n",
        "  - M represent the intepretent value of Y that's why we call it as Y interpreted, sometime we also call it as θ1 or β1. It shows the slope of the regression line.\n",
        "\n",
        "4. What does the intercept c represent in the equation Y=mX+c ?\n",
        "  - C represent the actual value of Y we call it as Y actual, c is the intersect value where regression line cuts at y axis. We represent it sometime as θ0 or β0.\n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression ?\n",
        "  - To calculate slope of simple linear regression there is a formula used for, y2 - y1 / x2 - x1 it gives us the slope for the line which is M.\n",
        "\n",
        "6. What is the purpose of the least squares method in Simple Linear Regression ?\n",
        "  - In one plot there could be the multiple regression line but to find out correct slope of line which should be the closer to the value ploted so the we could get the minimum error we use the least squred method.\n",
        "\n",
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "  - The coefficient of determination (R²) is a number between 0 and 1 that measures how well a statistical model predicts an outcome. You can interpret the R² as the proportion of variation in the dependent variable that is predicted by the statistical model.\n",
        "\n",
        "8. What is Multiple Linear Regression ?\n",
        "  - When there is more than one independent variable in that case it's called as Multiple Linear Regression.\n",
        "\n",
        "9. What is the main difference between Simple and Multiple Linear Regression ?\n",
        "  - In simple linear regression there are two variable one independent variable and one dependent vaariable but in multiple linear regression there are more than one independent variable.\n",
        "\n",
        "10. What are the key assumptions of Multiple Linear Regression ?\n",
        "  - There are two or more independent variables.\n",
        "  - These can be measured using either continuous or categorical means.\n",
        "  - The three or more variables of interest should have a linear relationship, which you can check by using a scatterplot.\n",
        "  - The data should have homoscedasticity.\n",
        "\n",
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model ?\n",
        "  - Heteroskedasticity refers to a situation where the variance of the residuals is unequal over a range of measured values. If heteroskedasticity exists, the population used in the regression contains unequal variance, the analysis results may be invalid.\n",
        "\n",
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity ?\n",
        "  - To fix multicollinearity, one can remove one of the highly correlated variables, combine them into a single variable, or use a dimensionality reduction technique such as principal component analysis to reduce the number of variables while retaining most of the information.\n",
        "\n",
        "13. What are some common techniques for transforming categorical variables for use in regression models ?\n",
        "  - One-Hot Encoding. One-hot encoding, also known as dummy encoding, is a popular technique for converting categorical data into a numerical format. ...\n",
        "  - Dummy Encoding. ...\n",
        "  - Effect Encoding. ...\n",
        "  - Label Encoding. ...\n",
        "  - Ordinal Encoding. ...\n",
        "  - Count Encoding. ...\n",
        "  - Binary Encoding.\n",
        "\n",
        "14. What is the role of interaction terms in Multiple Linear Regression ?\n",
        "  -  interaction terms enable you to examine whether the relationship between the target and the independent variable changes depending on the value of another independent variable.\n",
        "\n",
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression ?\n",
        "  - It is interpreted the same as a simple linear regression formula—except there are multiple variables that all impact the slope of the relationship.\n",
        "\n",
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions ?\n",
        "  - The slope of regression in species-area relationship predicts species richness of an area. It indicates the dependency of species richness on the area as higher slope reflects higher dependency of the area. Taking into account of a large area, such as country, the slope is almost linear with the area.\n",
        "\n",
        "17. How does the intercept in a regression model provide context for the relationship between variables ?\n",
        "  - The intercept or constant in the regression model represents the mean value of the response variable when all the predictor variables in the model are equal to zero. In linear regression, the intercept is the value of the dependent variable, i.e., Y when all values are independent variables, and Xs are zero.\n",
        "\n",
        "18. What are the limitations of using R² as a sole measure of model performance ?\n",
        "  -  it doesn't tell you whether your chosen model is good or bad, nor will it tell you whether the data and predictions are biased. A high or low R-squared isn't necessarily good or bad—it doesn't convey the reliability of the model or whether you've chosen the right regression.\n",
        "\n",
        "19. How would you interpret a large standard error for a regression coefficient ?\n",
        "  - A smaller standard error suggests that the estimate of the coefficient is more likely to be close to the true population value. In contrast, a larger standard error indicates more variability and less confidence in the precision of the coefficient estimate.\n",
        "\n",
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it ?\n",
        "  - A residual plot that displays a funnel shape (a widening or narrowing pattern) indicates heteroscedasticity. Such patterns necessitate remedial measures, which might include variable transformation (e.g., logarithmic, square root) or the use of weighted least squares methods.\n",
        "\n",
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "  - While r-squared measures the proportion of variance in the dependent variable explained by the independent variables, it always increases when more predictors are added. Adjusted r-squared adjusts for the number of predictors and decreases if the additional variables do not contribute to the model's significance.\n",
        "\n",
        "22. Why is it important to scale variables in Multiple Linear Regression ?\n",
        "  - In regression, it is often recommended to scale the features so that the predictors have a mean of 0. This makes it easier to interpret the intercept term as the expected value of Y when the predictor values are set to their means.\n",
        "\n",
        "23. What is polynomial regression ?\n",
        "  - In statistics, polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial in x.\n",
        "\n",
        "24. How does polynomial regression differ from linear regression ?\n",
        "  - polynomial regression has the curved line where the linear regression has the straight line of slope.\n",
        "\n",
        "25. When is polynomial regression used ?\n",
        "  - For cases where the data points are arranged in a non-linear fashion, there is a need for polynomial regression. If a non-linear model is present and you try to cover it using a linear model, it will cover no data points. Hence, a polynomial model is used to ensure that the data points are covered.\n",
        "\n",
        "26. What is the general equation for polynomial regression ?\n",
        "  - We are using polynomial regression when you predict Y using a single X variable together with some of its powers (X2, X3, etc.). Let us consider just the case of X with X2. With these variables, the usual multiple regression equation, Y = a + b1X1 + b2X2, becomes the quadratic polynomial Y = a + b1X + b2X2.\n",
        "\n",
        "27. Can polynomial regression be applied to multiple variables ?\n",
        "  - The Multivari- ate Polynomial Regression is used for value prediction when there are multiple values that contribute to the estimation of val- ues.\n",
        "\n",
        "28. What are the limitations of polynomial regression ?\n",
        "  - Overfitting: Higher-degree polynomial models are susceptible to overfitting, where the model fits the training data too closely and loses generalization ability.\n",
        "\n",
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial ?\n",
        "  - The solution is to have a separate validation set on which we can evaluate the model for each polynomial degree. If we fit the model using the original data (training set) but evaluate it using the validation set, the model will overfit to the noise in the training set but not in the validation set.\n",
        "\n",
        "30. Why is visualization important in polynomial regression ?\n",
        "  - Understanding the Data Fit\n",
        "  - Choosing the Right Polynomial Degree\n",
        "  - Detecting Overfitting and Underfitting\n",
        "  - Comparing Different Models\n",
        "  - Identifying Outliers and Trends\n",
        "\n",
        "31. How is polynomial regression implemented in Python ?\n",
        "  - Polynomial regression is implemented in Python using numpy for data manipulation and sklearn (Scikit-Learn) for modeling. It extends linear regression by adding polynomial features to the input data.\n",
        "  "
      ],
      "metadata": {
        "id": "VungVgTmkFqr"
      }
    }
  ]
}